{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random, Torch and Cuda settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 50\n",
    "random_seed = 40\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion, device, X_test):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_sobject_ids  = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    predicted_probs = []\n",
    "    conv_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, conv_out = model(data, return_conv=True)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            all_indices = (batch_idx * dataloader.batch_size) + np.arange(target.size(0))\n",
    "            all_sobject_ids.extend(X_test.iloc[all_indices].index.tolist())\n",
    "            true_labels.extend(target.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            predicted_probs.extend(F.softmax(output, dim=1).cpu().numpy())\n",
    "            conv_outputs.append(conv_out.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc, all_sobject_ids, true_labels, predicted_labels, predicted_probs, conv_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "The .csv file should contain two columns - sobject_id and label (1 for binary, 0 for single). The .h5 file should contain spectra converted from .fits files as shown in fits_2_h5.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(r\"path-to-labels.csv\")\n",
    "\n",
    "store = pd.HDFStore(f'path-to-file.h5')\n",
    "X_test = store['X_test']\n",
    "y_test = store['y_test'].values.flatten()\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "First set the run_count, which defines how many times you want to run the model (with a different random seed each time). Then choose the model architecture, all the models are listed in models.py. The trained models are saved in the folder models_saved. After the model goes through all the runs, the results are saved to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "class SpectraDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "test_dataset = SpectraDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_samples_df = pd.DataFrame()\n",
    "\n",
    "run_count = 5\n",
    "flux_length = 4096\n",
    "\n",
    "for run in range(run_count):\n",
    "\n",
    "    model = CNN_1a(flux_length)\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f'models_saved/{model.__class__.__name__[3:]}_run_{run + 1}.pth', map_location=torch.device('cuda')))\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc, test_sobject_ids, true_labels, predicted_labels, predicted_probs, conv_outputs = test(model, test_loader, criterion, device, X_test)\n",
    "\n",
    "    if run == 0:\n",
    "        test_samples_df = pd.DataFrame({\n",
    "            'sobject_id': test_sobject_ids,\n",
    "            'true_label': true_labels\n",
    "        })\n",
    "\n",
    "    test_samples_df[f'predicted_label_run_{run + 1}'] = predicted_labels\n",
    "    test_samples_df[f'predicted_prob_run_{run + 1}'] = [max(probs) for probs in predicted_probs]\n",
    "\n",
    "    print(f\"Run {run + 1} completed.\")\n",
    "\n",
    "test_samples_df.to_csv(f\"path-to-file.csv\", index=False)\n",
    "\n",
    "print('FINISHED!')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
