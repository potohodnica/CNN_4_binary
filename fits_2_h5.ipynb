{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as pyfits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os, urllib\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccd_dict = {\n",
    "# 1: \"B\",\n",
    "# 2: \"G\",\n",
    "  3: \"R\",\n",
    "# 4: \"I\"\n",
    "}\n",
    "\n",
    "ccd_list_keys = list(ccd_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read fits files\n",
    "This code is mainly based on GALAH existing code found at https://github.com/svenbuder/GALAH_DR3/tree/master/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_spectra(sobject_id, ccd_list_keys):\n",
    "    \"\"\"\n",
    "    Read in all available CCDs and give back a dictionary\n",
    "    Download them if not already in working directory\n",
    "    \"\"\"\n",
    "    spectra_directory = \"fits-path\"\n",
    "    # Check if FITS files already available in working directory\n",
    "    fits_files = [[], [], [], []]\n",
    "    for each_ccd in ccd_list_keys:\n",
    "        fits_files[each_ccd-1] = glob.glob(spectra_directory+str(sobject_id)+str(each_ccd)+'.fits')  \n",
    "    spectrum = dict()\n",
    "\n",
    "    for each_ccd in ccd_list_keys:\n",
    "        if fits_files[each_ccd-1]!=[]:\n",
    "\n",
    "            fits = pyfits.open(fits_files[each_ccd-1][0])\n",
    "\n",
    "            # Extension 0: Reduced spectrum\n",
    "            # Extension 1: Relative error spectrum\n",
    "            # Extension 4: Normalised spectrum, NB: cut for CCD4\n",
    "\n",
    "            # Extract wavelength grid for the normalised spectrum\n",
    "\n",
    "            start_wavelength = fits[4].header[\"CRVAL1\"]\n",
    "            dispersion       = fits[4].header[\"CDELT1\"]\n",
    "            nr_pixels        = fits[4].header[\"NAXIS1\"]\n",
    "            reference_pixel  = fits[4].header[\"CRPIX1\"]\n",
    "            if reference_pixel == 0:\n",
    "                reference_pixel=1\n",
    "            spectrum['wave_norm_'+str(each_ccd)] = ((np.arange(0,nr_pixels)--reference_pixel+1)*dispersion+start_wavelength)\n",
    "\n",
    "            # Extract flux and flux error of reduced spectrum\n",
    "            # Added byteswap for Pandas use ----> https://stackoverflow.com/questions/30283836/creating-pandas-dataframe-from-numpy-array-leads-to-strange-errors\n",
    "            spectrum['sob_red_'+str(each_ccd)]  = np.array(fits[0].data).byteswap().newbyteorder()\n",
    "            # Extract flux and flux error of normalised spectrum\n",
    "            spectrum['sob_norm_'+str(each_ccd)] = np.array(fits[4].data)\n",
    "            spectrum['uob_norm_'+str(each_ccd)] = np.array(fits[4].data * fits[1].data)\n",
    "            fits.close()\n",
    "        else:\n",
    "            spectrum['wave_red_'+str(each_ccd)] = []\n",
    "            spectrum['wave_norm_'+str(each_ccd)] = []\n",
    "            spectrum['sob_red_'+str(each_ccd)] = []\n",
    "            spectrum['sob_norm_'+str(each_ccd)] = []\n",
    "            spectrum['uob_red_'+str(each_ccd)] = []\n",
    "            spectrum['uob_norm_'+str(each_ccd)] = []\n",
    "    \n",
    "    spectrum['sob_red'] = np.concatenate(([spectrum['sob_red_'+str(each_ccd)] for each_ccd in ccd_list_keys]))\n",
    "    spectrum['sob_norm'] = np.concatenate(([spectrum['sob_norm_'+str(each_ccd)] for each_ccd in ccd_list_keys]))\n",
    "    spectrum['wave_norm'] = np.concatenate(([spectrum['wave_norm_'+str(each_ccd)] for each_ccd in ccd_list_keys]))\n",
    "    spectrum['uob_norm'] = np.concatenate(([spectrum['uob_norm_'+str(each_ccd)] for each_ccd in ccd_list_keys]))\n",
    "   \n",
    "    return spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using code above convert .fits files to .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"input-fits-files-path\"\n",
    "output_dir = r\"ouput-csv-files-path\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "fits_files = glob.glob(os.path.join(input_dir, \"*.fits\"))\n",
    "total_files = len(fits_files)\n",
    "\n",
    "# Loop through the FITS files, convert them to CSV, and save them in the output directory\n",
    "for i, fits_file in enumerate(fits_files):\n",
    "\n",
    "    file_name_without_ext = os.path.splitext(os.path.basename(fits_file))[0][:-1]\n",
    "    spectrum = read_spectra(file_name_without_ext, ccd_list_keys)\n",
    "    my_array = spectrum['sob_norm_3']\n",
    "    flattened_array = np.sort(my_array.flatten())\n",
    "    differences = np.diff(flattened_array)\n",
    "    min_difference = np.min(differences[differences > 0])\n",
    "    decimal_points = int(np.ceil(-np.log10(min_difference)))\n",
    "\n",
    "    try: \n",
    "        csv_file = os.path.join(output_dir, file_name_without_ext + \".csv\")\n",
    "        fmt_str = f'%.{decimal_points}f'\n",
    "        np.savetxt(csv_file, my_array, delimiter=',', fmt=fmt_str)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing file: {fits_file}\\nError: {e}\\n\")\n",
    "\n",
    "    print(f\"Processed file {i + 1} of {total_files}\", end='\\r')\n",
    "    \n",
    "print(\"All files have been converted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate logarithm of wavelengths and interpolate intensity\n",
    "Done because of the Doppler shift, otherwise the bottoms of the double lines would become more and more spaced as the wavelength increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"csv-files-path\"\n",
    "target_dir = \"logged-csv-files-path\"\n",
    "\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    target_subdir = root.replace(source_dir, target_dir)\n",
    "\n",
    "    if os.path.exists(target_subdir):\n",
    "        continue\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                wavelength = df['wave_norm_3']\n",
    "                intensity = df['sob_norm_3']\n",
    "                log_wavelength = np.log(wavelength)\n",
    "                new_log_wavelength = np.linspace(log_wavelength.min(), log_wavelength.max(), num=len(log_wavelength))\n",
    "\n",
    "                interpolation_function = interp1d(log_wavelength, intensity, kind='linear')\n",
    "                new_intensity = interpolation_function(new_log_wavelength)\n",
    "\n",
    "                interpolated_df = pd.DataFrame({'wave_norm_3': new_log_wavelength, 'sob_norm_3': new_intensity})\n",
    "\n",
    "                os.makedirs(target_subdir, exist_ok=True)\n",
    "\n",
    "                interpolated_df.to_csv(os.path.join(target_subdir, file), index=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for loading logged .csv files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sobject_id):\n",
    "    folder_name = str(sobject_id)[:6]\n",
    "\n",
    "    file_path = f\"logged-csv-files-path\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        sob_df = pd.DataFrame({str(sobject_id): df['sob_norm_3']})\n",
    "        wave_df = pd.DataFrame({str(sobject_id): df['wave_norm_3']})\n",
    "        return sob_df, wave_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of parallel for faster loading of .csv files in the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(r\"path-to-labels.csv\")\n",
    "selected_sobject_ids = labels_df['sobject_id'].values\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(load_data)(sobject_id) for sobject_id in selected_sobject_ids)\n",
    "\n",
    "spektri_df = pd.concat([res[0] for res in results if res is not None], axis=1)\n",
    "wave_df = pd.concat([res[1] for res in results if res is not None], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the loaded .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map sobject_id to label\n",
    "id_to_label = dict(zip(labels_df['sobject_id'], labels_df['bin_tf']))\n",
    "\n",
    "spektri_df = spektri_df.dropna()  # Remove rows containing NaN values\n",
    "X = spektri_df.T\n",
    "X.index = X.index.map(lambda x: int(float(x)))  # Remove decimal point and convert to int\n",
    "X.index = X.index.astype(np.int64)\n",
    "X = X.sort_index()\n",
    "\n",
    "id_to_label = {np.int64(k): v for k, v in id_to_label.items()}\n",
    "X = X[X.index.isin(id_to_label.keys())]\n",
    "y = [id_to_label[sobject_id] for sobject_id in X.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, split the data into train (80%) and test (20%) sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "# Then, split the train set into train (75%) and validation (25%) sets. This results in a 60%-20%-20% split.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=seed)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class SpectraDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = SpectraDataset(X_train, y_train)\n",
    "val_dataset = SpectraDataset(X_val, y_val)\n",
    "test_dataset = SpectraDataset(X_test, y_test)\n",
    "\n",
    "# Create PyTorch dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to .h5 file type\n",
    "This enables much faster load times when running or training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.HDFStore(r'path-to-file.h5')\n",
    "store['X_train'] = X_train\n",
    "store['X_val'] = X_val\n",
    "store['X_test'] = X_test\n",
    "store['y_train'] = pd.Series(y_train)\n",
    "store['y_val'] = pd.Series(y_val)\n",
    "store['y_test'] = pd.Series(y_test)\n",
    "store.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
